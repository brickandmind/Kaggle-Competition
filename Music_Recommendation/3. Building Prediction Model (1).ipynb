{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우와 RandomForest를 사용하여 모델을 만들어 봤습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv('X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "['X_train'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [[0.] for cl in range(103)]\n",
    "xy = tf.decode_csv(value, record_defaults = record_defaults)\n",
    "train_x_batch, train_y_batch = tf.train.batch([xy[0:-2], xy[-2:]], batch_size=100)\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 101], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 2], name = 'y-input')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape=[101, 50],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([50]), name='bias1')\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=0.5)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [50, 70],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([70]), name='bias2')\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    \n",
    "L2 = tf.nn.dropout(L2, keep_prob=0.5)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [70, 30],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([30]), name='bias3')\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    \n",
    "L3 = tf.nn.dropout(L3, keep_prob=0.5)\n",
    "\n",
    "\n",
    "\n",
    "W4 = tf.get_variable(\"weight4\", shape = [30, 15],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([15]), name='bias4')\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    \n",
    "L4 = tf.nn.dropout(L4, keep_prob=0.5)\n",
    "\n",
    "\n",
    "W5 = tf.get_variable(\"weight5\", shape = [15, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]), name='bias5')\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\n",
    "    \n",
    "L5 = tf.nn.dropout(L5, keep_prob=0.5)\n",
    "\n",
    "\n",
    "W6 = tf.get_variable(\"weight6\", shape = [10, 20],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([20]), name='bias6')\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6)\n",
    "    \n",
    "L6 = tf.nn.dropout(L6, keep_prob=0.5)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "W7 = tf.get_variable(\"weight7\", shape = [20, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([10]), name='bias7')\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\n",
    "    \n",
    "L7 = tf.nn.dropout(L7, keep_prob=0.5)\n",
    "    \n",
    "\n",
    "W8 = tf.get_variable(\"weight8\", shape = [10, 2],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([2]), name='bias8')\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess= tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, save_path = '/practice/my_model', global_step = 10000)\n",
    "    \n",
    "for step in range(3000000):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    sess.run( train, feed_dict={X: x_batch, Y: y_batch})\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={\n",
    "                X: x_batch, Y: y_batch}))\n",
    "#coord.request_stop()\n",
    "#coord.join(threads)\n",
    "\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test')\n",
    "x_data = X_test.iloc[:, :-2].values\n",
    "y_data = X_test.iloc[:, -2:].values\n",
    "y_data = np.reshape(y_data, (-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.650303\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: x_data, Y: y_data,  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우의 경우 learn rate, batch size, number of layer를 중점으로 모델을 수정하며 평균적으로 1시간 반 정도 학습 시켜보았지만 cost가 0.60 아래로  내려가지 않았습니다. 그 결과 저는 다음과 같은 문제가 있을 거라고 생각 하였습니다\n",
    "\n",
    "1. input 변수들이 옳바르지 않거나 더 세부적인 필터링이 필요하다\n",
    "2. 텐서플로우의 모델이 잘못되었다\n",
    "3. 더 많은 학습 시간이 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, precision_recall_curve, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "             vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "      # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    " \n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                                img.get_array()):\n",
    "        \n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            c = 'k'\n",
    "        else:\n",
    "             c = 'w'\n",
    "        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_gridRF = {'max_features' : [1, 10 ,100],\n",
    "                'min_samples_leaf' : [1, 10, 100]}\n",
    "grid_searchRF = GridSearchCV(RandomForestClassifier(n_jobs = -1), param_gridRF, cv = 10)\n",
    "grid_searchRF.fit(X_train[:, -1], y_train)\n",
    "print(format(grid_searchRF.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsRF = pd.DataFrame(grid_searchRF.cv_results_)\n",
    "scoresRF = np.array(resultsRF.mean_test_score).reshape(3,3)\n",
    "%matplotlib inline\n",
    "heatmap(scoresRF, xlabel='max_features', xticklabels=param_gridRF['max_features'], ylabel='min_samples_leaf', \n",
    "        yticklabels=param_gridRF['min_samples_leaf'], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_jobs = -1, man_features = 10, min_samples_leaf = 10)\n",
    "clf_RF.fit(X_train[:, -1], y_train)\n",
    "print('Rf accuracy:', clf_RF.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest를 그리드 서치를 통해 max_geatures 과 min_sample_leaf를 변환시켜 학습 시켜 본 결과 각 각 10의 값을 가질 떄 가장 높은 정확도를 가진다는 것을 알 수 있습니다. 평균적으로 69%의 정확도가 나오고 kaggle의 리더보드에서 1등을 한 사람의 모델이 71%의 정확도를 보인다는 것을 볼 때 딥러닝 모델보다 짧은 시간 안에(대략 20분) 더 낳은 결과를 예측 하다는 것에 있어서 매우 만족스러운 모델 이라고 할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
